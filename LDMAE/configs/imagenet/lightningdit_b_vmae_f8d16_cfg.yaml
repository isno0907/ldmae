# we recommend to read config_details.yaml first.

ckpt_path: 'output/imagenet/lightningdit_b_vmae_f8d16/checkpoints/0100000.pt' # <---- download our pre-trained lightningdit or your own checkpoint

data:
  origin_path: '/data/dataset/imagenet/1K_dataset'
  data_path: '/data/dataset/imagenet/vmae_feature_imagenet_train_256' # <---- path to your data. it is generated by extract_features.py.
                                 #       if you just want to inference, download our latent_stats.pt and give its path here is ok.
  fid_reference_file: 'tools/fid_statistics/VIRTUAL_imagenet256_labeled.npz' # <---- path to your fid_reference_file.npz. download it from ADM

  # recommend to use default settings
  image_size: 256
  num_classes: 1000
  num_workers: 8
  latent_norm: true
  latent_multiplier: 1.0
  sample: true  # <------------------------------ check this. you should comment out this when you don't want to use it.

# recommend to use default settings (we wil release codes with SD-VAE later)
vae:
  model_name: 'vmae_f8d16'
  downsample_ratio: 8
  weight_path: 'pretrain_weight/vmaef8d16.pth'

# recommend to use default settings
model:
  model_type: LightningDiT-B/1
  use_qknorm: true
  use_swiglu: true
  use_rope: true
  use_rmsnorm: true
  wo_shift: false
  in_chans: 16

# recommend to use default settings
train:
  max_steps: 100000
  global_batch_size: 256 # 256 ok
  global_seed: 0
  output_dir: 'output'
  exp_name: 'imagenet/lightningdit_b_vmae_f8d16' # <---- experiment name, set as you like
  ckpt: null
  log_every: 100
  ckpt_every: 20000
  use_checkpoint: false
  gradient_accumulation_steps: 1
optimizer:
  lr: 0.0002
  beta2: 0.95
  # max_grad_norm: 1.0
# recommend to use default settings
transport:
  path_type: Linear
  prediction: velocity
  loss_weight: null
  sample_eps: null
  train_eps: null
  use_cosine_loss: false
  use_lognorm: true

# recommend to use default settings
sample:
  mode: ODE
  sampling_method: euler
  atol: 0.000001
  rtol: 0.001
  reverse: false
  likelihood: false
  num_sampling_steps: 250
  cfg_scale: 10.0 # <---- cfg scale, for 800 epoch performance with FID=1.35 cfg_scale=6.7
                  #       for 64 epoch performance with FID=2.11 cfg_scale=10.0
                  #       you may find we use a large cfg_scale, this is because of 2 reasons:
                  #       we find a high-dimensional latent space requires a large cfg_scale to get good performance than f8d4 SD-VAE
                  #       we enable cfg interval, which reduces the negative effects of large cfg on high-noise parts. This means larger cfg can be utilized
  
  # recommend to use default settings
  per_proc_batch_size: 256
  fid_num: 50000
  cfg_interval_start: 0.10
  timestep_shift: 0.3
